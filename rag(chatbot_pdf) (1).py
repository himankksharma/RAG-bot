# -*- coding: utf-8 -*-
"""RAG(CHATBOT_PDF).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1E3PFXkkjI9NMz6408AV3pPP3Bhhuv7ak
"""

!pip install pymupdf faiss-cpu sentence-transformers transformers

import fitz
import faiss
import numpy as np

def read_text(doc):
    text = ""
    for page in doc:
        text += page.get_text()
    return text

doc=fitz.open(r"/content/Brene-brown-book1.pdf")
raw_text = read_text(doc)

raw_text

def split_into_chunks(text, chunk_size=500):
    chunks = []
    for i in range(0, len(text), chunk_size):
        chunk = text[i:i + chunk_size]
        chunks.append(chunk)
    return chunks

from sentence_transformers import SentenceTransformer

model=SentenceTransformer("all-MiniLM-L6-v2")

chunks = split_into_chunks(raw_text)

embeddings=model.encode(chunks)

#  Convert list of embeddings into a NumPy array (2D)
embedding_array = np.array(embeddings).astype("float32")

#  Just to be safe: reshape if it's only 1D
if len(embedding_array.shape) == 1:
    embedding_array = embedding_array.reshape(1, -1)

#  Create a FAISS index
dimension = embedding_array.shape[1]  # size of each vector (should be 384)
index = faiss.IndexFlatL2(dimension)

# Add your embeddings to it
index.add(embedding_array)


print("Chunks stored in FAISS:", index.ntotal)

# print(len(chunks))
print(chunks[0] if len(chunks) > 0 else "No chunks found")

print(len(embeddings))
print(type(embeddings[0]))

print(index.ntotal)

def get_context(question, top_k=1):
    question_embedding = model.encode([question]).astype("float32")
    print("Shape of question embedding:", question_embedding.shape)

    distances, indices = index.search(question_embedding, top_k)
    print("Indices returned:", indices)

    valid_indices = [i for i in indices[0] if i != -1]

    if not valid_indices:
        return "No relevant context found."

    results = [chunks[i] for i in valid_indices]
    return "\n\n".join(results)

from transformers import pipeline

qa_model = pipeline("text-generation", model="gpt2")

def chatbot(query):
    context = get_context(query)
    prompt = f"Answer based on this:\n{context}\n\nQ: {query}\nA:"
    response = qa_model(prompt, max_length=150, do_sample=True)[0]['generated_text']
    return response

print("ğŸ¤– Hello! I'm your PDF Assistant. Ask me anything from the document.")
print("Type 'exit' or 'quit' anytime to end the chat.\n")

while True:
    user_input = input("ğŸ‘¤ You: ")
    if user_input.strip().lower() in ["exit", "quit"]:
        print("ğŸ¤– Bot: Goodbye! ğŸ‘‹")
        break

    response = chatbot(user_input)

    if response.strip() == "":
        print("ğŸ¤– Bot: Hmm... I couldn't find an answer in the document. Try rephrasing?")
    else:
        print(f"ğŸ¤– Bot: {response.strip()}\n")

import gradio as gr
import fitz
import faiss
import numpy as np
from sentence_transformers import SentenceTransformer

# Load model
model = SentenceTransformer("all-MiniLM-L6-v2")

# Globals
chunks = []
index = None

# Chatbot logic
def chatbot_interface(query):
    if query.strip().lower() in ["exit", "quit"]:
        return "ğŸ‘‹ Thanks for chatting! See you soon."

    response = get_response(query)
    return response.strip() if response.strip() else "ğŸ¤– Sorry, I couldn't find an answer. Try rephrasing?"

# Search in PDF chunks
def get_response(query):
    global chunks, index
    q_embedding = model.encode([query]).astype("float32")
    distances, indices = index.search(q_embedding, k=1)
    context = chunks[indices[0][0]]
    return f"ğŸ“– Context:\n{context}\n\nğŸ’¬ Answer (dummy): GPT-2 would answer here."

# âœ… Fix is applied here: using `fitz.open(file)` instead of `.read()`
def process_pdf(file):
    try:
        global chunks, index
        doc = fitz.open(file)  # âœ… Fix: pass file path directly

        text = ""
        for page in doc:
            text += page.get_text()
        doc.close()

        chunks = [text[i:i + 500] for i in range(0, len(text), 500)]
        embeddings = model.encode(chunks).astype("float32")
        index = faiss.IndexFlatL2(embeddings.shape[1])
        index.add(embeddings)

        return "âœ… PDF uploaded and processed successfully!"
    except Exception as e:
        import traceback
        print(traceback.format_exc())
        return f"âŒ Error: {str(e)}"

# CSS (replace with your actual CSS if needed)
css = """
body {
    background: linear-gradient(to right, #e0eafc, #cfdef3);
    animation: fadeIn 1s ease-in;
}
h1, h2, p {
    font-family: 'Segoe UI', sans-serif !important;
    color: #2c3e50;
    text-align: center;
}
.gradio-container {
    font-family: 'Segoe UI', sans-serif;
    padding: 20px;
}
textarea {
    font-size: 16px !important;
    border-radius: 12px !important;
    border: 1px solid #dcdde1 !important;
    box-shadow: 0 0 5px rgba(0,0,0,0.1) !important;
    transition: all 0.3s ease-in-out;
}
textarea:focus {
    border-color: #3498db !important;
    box-shadow: 0 0 8px #3498db50 !important;
}
button {
    background-color: #3498db !important;
    color: white !important;
    font-size: 16px !important;
    border-radius: 10px !important;
    padding: 10px 20px !important;
    transition: background-color 0.3s ease;
}
button:hover {
    background-color: #2980b9 !important;
}
@keyframes fadeIn {
    0% { opacity: 0; transform: translateY(10px); }
    100% { opacity: 1; transform: translateY(0); }
}
"""

# Gradio interface
with gr.Blocks(css=css) as demo:
    gr.Markdown("## ğŸ“š AI PDF Chatbot")
    gr.Markdown("""
    <div style='text-align: center; font-size: 16px; margin-top: 10px;'>
        ğŸ¤– Ask questions based on your PDF.<br>
        Uses <strong>FAISS</strong> for context & <strong>GPT-2</strong> to generate answers.<br>
        Upload a PDF first, then start chatting!
    </div>
    """)

    with gr.Row():
        pdf_input = gr.File(label="ğŸ“„ Upload your PDF", file_types=[".pdf"])
        upload_btn = gr.Button("ğŸ“¤ Process PDF")

    upload_status = gr.Textbox(label="Status", interactive=False)
    upload_btn.click(fn=process_pdf, inputs=pdf_input, outputs=upload_status)

    with gr.Row():
        question = gr.Textbox(lines=3, label="ğŸ“Œ Your Question", placeholder="Ask something from the uploaded PDF...")
        answer = gr.Textbox(label="ğŸ¤– Bot's Answer")
    ask_btn = gr.Button("ğŸ’¬ Ask")  # <-- Added Ask button
    ask_btn.click(fn=chatbot_interface, inputs=question, outputs=answer)


demo.launch(share=True, debug=True)

